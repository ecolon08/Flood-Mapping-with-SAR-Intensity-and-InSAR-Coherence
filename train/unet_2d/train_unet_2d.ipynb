{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Improving Semantic Water Segmentation by Fusing Sentinel-1 Intensity and Interferometric Synthetic Aperture Radar (InSAR) Coherence Data\n",
    "\n",
    "**Author: Ernesto Colon**\n",
    "**The Cooper Union for the Advancement of Science and Art**\n",
    "\n",
    "#### Unet-2D Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Sun Mar 13 12:35:18 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.11       Driver Version: 471.11       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 32%   47C    P2    46W / 350W |   1777MiB / 24576MiB |     11%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1892    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      5068    C+G   ...ekyb3d8bbwe\\onenoteim.exe    N/A      |\n",
      "|    0   N/A  N/A      5172    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A      7796    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      7880    C+G   ...1\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    0   N/A  N/A     10680    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14092    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14244    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     14260    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     15476    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     16596    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     19868    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     19904    C+G   ...eripheral Manager\\DPM.exe    N/A      |\n",
      "|    0   N/A  N/A     20320    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     21168    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21200      C   ...da3\\envs\\tf_rs\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     21904    C+G   ...4__8wekyb3d8bbwe\\Time.exe    N/A      |\n",
      "|    0   N/A  N/A     22208    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     22332    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import dataset_gen\n",
    "\n",
    "# check that a GPU is enabled\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "script_start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the train, validation, and test dataframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define dictionary with filepaths\n",
    "base_dir = r\"C:\\Users\\ernes\\OneDrive - The Cooper Union for the Advancement of Science and Art\\Thesis-DESKTOP-UID4081\\flood_mapping\\10m_resolution\"\n",
    "\n",
    "train_val_test_pths = {'train_fn_df' : f\"{base_dir}\\\\ds_train_split_10m.csv\",\n",
    "                       'val_fn_df' : f\"{base_dir}\\\\ds_val_split_10m.csv\",\n",
    "                       'test_fn_df' : f\"{base_dir}\\\\ds_test_split_10m.csv\"}\n",
    "\n",
    "train_val_fn_df, test_fn_df, train_size, val_size, test_size =\\\n",
    "    dataset_gen.unet_load_ds_df(train_val_test_pths['train_fn_df'],\n",
    "                                train_val_test_pths['val_fn_df'],\n",
    "                                train_val_test_pths['test_fn_df'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Define dictionaries to hold the datasets - the keys will be the different scenarios\n",
    "X_train_dict = {}\n",
    "Y_train_dict = {}\n",
    "\n",
    "X_val_dict = {}\n",
    "Y_val_dict = {}\n",
    "\n",
    "X_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "Y_pred_dict = {}\n",
    "\n",
    "# Define scenario number to scenario name mapping\n",
    "scenario_dict = {1: 'co_event_intensity_only',\n",
    "                 2: 'pre_co_event_intensity',\n",
    "                 3: 'pre_co_event_int_coh'}\n",
    "\n",
    "scenario_num_bands = {1: 2,\n",
    "                      2: 4,\n",
    "                      3: 6}\n",
    "\n",
    "# Define the number of bands per scenario\n",
    "num_bands_dict = {'co_event_intensity_only': 2,\n",
    "                 'pre_co_event_intensity': 4,\n",
    "                 'pre_co_event_int_coh': 6}\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "# define dictionaries to hold the datasets\n",
    "train_val_samples_dict = {}\n",
    "test_samples_dict = {}\n",
    "\n",
    "scenarios = [1, 2, 3]\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    # Create the samples list given the dataframes with file paths as input\n",
    "    train_val_samples_dict[f\"scenario_{scenario}\"], test_samples_dict[f\"scenario_{scenario}\"] = \\\n",
    "        dataset_gen.create_samples_list({'scenario': scenario_dict[scenario],\n",
    "                                            'test_df': test_fn_df,\n",
    "                                            'train_val_df': train_val_fn_df})\n",
    "\n",
    "    # Create data sets dictionary\n",
    "    X_train_dict[f\"scenario_{scenario}\"], X_val_dict[f\"scenario_{scenario}\"], X_test_dict[f\"scenario_{scenario}\"] =\\\n",
    "        dataset_gen.unet_ds_creation({'train_val_list': train_val_samples_dict[f\"scenario_{scenario}\"],\n",
    "                                      'test_list': test_samples_dict[f\"scenario_{scenario}\"]})\n",
    "\n",
    "    # Batch the tensorflow train, val, and test data set generators\n",
    "    X_train_dict[f\"scenario_{scenario}\"] = X_train_dict[f\"scenario_{scenario}\"].batch(5).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    X_val_dict[f\"scenario_{scenario}\"] = X_val_dict[f\"scenario_{scenario}\"].batch(5).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    X_test_dict[f\"scenario_{scenario}\"] = X_test_dict[f\"scenario_{scenario}\"].batch(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying out the attention U-net from this repo:\n",
    "\n",
    "https://github.com/yingkaisha/keras-unet-collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from keras_unet_collection import models\n",
    "# create dictionary to hold the models by scenario\n",
    "unet_2d_models = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 1\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n",
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 2\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n",
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 3\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# loop through scenarios and generate the models\n",
    "for scenario in scenarios:\n",
    "    # Create models for each scenario\n",
    "    print(\"\\n*******************************************\\n\")\n",
    "    print(f\"Generating model for scenario: {scenario}\")\n",
    "    unet_2d_models[f\"scenario_{scenario}\"] = models.u2net_2d((IMG_SIZE, IMG_SIZE, scenario_num_bands[scenario]),\n",
    "                                                             n_labels=2,\n",
    "                                                             filter_num_down=[64, 128, 256, 512, 1024],\n",
    "                                                             filter_num_up='auto',\n",
    "                                                             filter_mid_num_down='auto',\n",
    "                                                             filter_mid_num_up='auto',\n",
    "                                                             filter_4f_num='auto',\n",
    "                                                             filter_4f_mid_num='auto',\n",
    "                                                             activation='ReLU',\n",
    "                                                             output_activation='Sigmoid',\n",
    "                                                             batch_norm=False,\n",
    "                                                             pool=True,\n",
    "                                                             unpool=True,\n",
    "                                                             deep_supervision=False,\n",
    "                                                             name='u2net')\n",
    "    print(\"*******************************************\")\n",
    "\n",
    "#unet_2d_models['scenario_3'].summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.0001,\n",
    "                                                             decay_steps=200,\n",
    "                                                             decay_rate=0.96,\n",
    "                                                             staircase=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     epsilon=1e-07,\n",
    "                                     amsgrad=False,\n",
    "                                     name='Adam')\n",
    "\n",
    "unet_2d_train_hist = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 1 - Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16/444 [>.............................] - ETA: 3:54 - loss: 0.3758 - accuracy: 0.8833"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_21200/2137345563.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mEPOCHS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0munet_2d_train_hist\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34mf\"scenario_{current_scenario}\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n\u001B[0m\u001B[0;32m     13\u001B[0m                                                \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mX_val_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34mf\"scenario_{current_scenario}\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m                                                epochs=EPOCHS)\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1187\u001B[0m               \u001B[0mlogs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtmp_logs\u001B[0m  \u001B[1;31m# No error, now safe to assign to logs.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1188\u001B[0m               \u001B[0mend_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep_increment\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1189\u001B[1;33m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mend_step\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1190\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstop_training\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1191\u001B[0m                 \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36mon_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    433\u001B[0m     \"\"\"\n\u001B[0;32m    434\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_should_call_train_batch_hooks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 435\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mModeKeys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTRAIN\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'end'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    436\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    437\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mon_test_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook\u001B[1;34m(self, mode, hook, batch, logs)\u001B[0m\n\u001B[0;32m    293\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_begin_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    294\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mhook\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'end'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 295\u001B[1;33m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_end_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    296\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    297\u001B[0m       \u001B[1;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Unrecognized hook: {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_end_hook\u001B[1;34m(self, mode, batch, logs)\u001B[0m\n\u001B[0;32m    313\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_batch_times\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_time\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    314\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 315\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_batch_hook_helper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhook_name\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    316\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    317\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_batch_times\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_batches_for_timing_check\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_call_batch_hook_helper\u001B[1;34m(self, hook_name, batch, logs)\u001B[0m\n\u001B[0;32m    351\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mcallback\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    352\u001B[0m       \u001B[0mhook\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcallback\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 353\u001B[1;33m       \u001B[0mhook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    354\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    355\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_check_timing\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36mon_train_batch_end\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1026\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1027\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mon_train_batch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1028\u001B[1;33m     \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_batch_update_progbar\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1029\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1030\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mon_test_batch_end\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlogs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\callbacks.py\u001B[0m in \u001B[0;36m_batch_update_progbar\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mverbose\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1099\u001B[0m       \u001B[1;31m# Only block async when verbose = 1.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1100\u001B[1;33m       \u001B[0mlogs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msync_to_numpy_or_python_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1101\u001B[0m       \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprogbar\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mseen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlogs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfinalize\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1102\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001B[0m in \u001B[0;36msync_to_numpy_or_python_type\u001B[1;34m(tensors)\u001B[0m\n\u001B[0;32m    514\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    515\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 516\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmap_structure\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensors\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    517\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    518\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36mmap_structure\u001B[1;34m(func, *structure, **kwargs)\u001B[0m\n\u001B[0;32m    867\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    868\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 869\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    870\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    871\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    867\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    868\u001B[0m   return pack_sequence_as(\n\u001B[1;32m--> 869\u001B[1;33m       \u001B[0mstructure\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mentries\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    870\u001B[0m       expand_composites=expand_composites)\n\u001B[0;32m    871\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\utils\\tf_utils.py\u001B[0m in \u001B[0;36m_to_single_numpy_or_python_type\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m    510\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_to_single_numpy_or_python_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    511\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 512\u001B[1;33m       \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    513\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitem\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    514\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m  \u001B[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mnumpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1092\u001B[0m     \"\"\"\n\u001B[0;32m   1093\u001B[0m     \u001B[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1094\u001B[1;33m     \u001B[0mmaybe_arr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1095\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmaybe_arr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mmaybe_arr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1096\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_numpy\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1058\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0m_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1059\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1060\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_numpy_internal\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1061\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1062\u001B[0m       \u001B[0msix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mraise_from\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_status_to_exception\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# pylint: disable=protected-access\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "current_scenario = 1\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 1\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_model_pth = r\"D:\\SAR_data\\Models\\refactored\\10m_res\\unet_2d\"\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 2 - Pre-event and Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 2\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights for scenario 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 3 - Pre-event and Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[10,192,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/model_2/u2net_up_4_in_0/Conv2D/Conv2DBackpropInput (defined at \\AppData\\Local\\Temp/ipykernel_21864/4261610726.py:12) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_27876]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mResourceExhaustedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_21864/4261610726.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mEPOCHS\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m30\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0munet_2d_train_hist\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34mf\"scenario_{current_scenario}\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m     unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n\u001B[0m\u001B[0;32m     13\u001B[0m                                                \u001B[0mvalidation_data\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mX_val_dict\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34mf\"scenario_{current_scenario}\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m                                                epochs=EPOCHS)\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\keras\\engine\\training.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   1182\u001B[0m                 _r=1):\n\u001B[0;32m   1183\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1184\u001B[1;33m               \u001B[0mtmp_logs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1185\u001B[0m               \u001B[1;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1186\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    883\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    884\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0mOptionalXlaContext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_jit_compile\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 885\u001B[1;33m         \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    886\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    887\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperimental_get_tracing_count\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001B[0m in \u001B[0;36m_call\u001B[1;34m(self, *args, **kwds)\u001B[0m\n\u001B[0;32m    948\u001B[0m         \u001B[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    949\u001B[0m         \u001B[1;31m# stateless function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 950\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    951\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    952\u001B[0m       \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfiltered_flat_args\u001B[0m \u001B[1;33m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   3037\u001B[0m       (graph_function,\n\u001B[0;32m   3038\u001B[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001B[1;32m-> 3039\u001B[1;33m     return graph_function._call_flat(\n\u001B[0m\u001B[0;32m   3040\u001B[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001B[0;32m   3041\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[1;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[0;32m   1961\u001B[0m         and executing_eagerly):\n\u001B[0;32m   1962\u001B[0m       \u001B[1;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1963\u001B[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0m\u001B[0;32m   1964\u001B[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0;32m   1965\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001B[0m in \u001B[0;36mcall\u001B[1;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[0;32m    589\u001B[0m       \u001B[1;32mwith\u001B[0m \u001B[0m_InterpolateFunctionError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    590\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcancellation_manager\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 591\u001B[1;33m           outputs = execute.execute(\n\u001B[0m\u001B[0;32m    592\u001B[0m               \u001B[0mstr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    593\u001B[0m               \u001B[0mnum_outputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_outputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\tf_rs\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[0;32m     57\u001B[0m   \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 59\u001B[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0m\u001B[0;32m     60\u001B[0m                                         inputs, attrs, num_outputs)\n\u001B[0;32m     61\u001B[0m   \u001B[1;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mResourceExhaustedError\u001B[0m:  OOM when allocating tensor with shape[10,192,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node gradient_tape/model_2/u2net_up_4_in_0/Conv2D/Conv2DBackpropInput (defined at \\AppData\\Local\\Temp/ipykernel_21864/4261610726.py:12) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_27876]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "current_scenario = 3\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights for scenario 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}