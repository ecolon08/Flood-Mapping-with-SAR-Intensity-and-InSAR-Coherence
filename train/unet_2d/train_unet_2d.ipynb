{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Improving Semantic Water Segmentation by Fusing Sentinel-1 Intensity and Interferometric Synthetic Aperture Radar (InSAR) Coherence Data\n",
    "\n",
    "**Author: Ernesto Colon**\n",
    "**The Cooper Union for the Advancement of Science and Art**\n",
    "\n",
    "#### Unet-2D Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n",
      "Sat Mar 12 12:54:25 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 471.11       Driver Version: 471.11       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 33%   34C    P2    53W / 350W |   2655MiB / 24576MiB |     12%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2996    C+G   ...e\\root\\Office16\\EXCEL.EXE    N/A      |\n",
      "|    0   N/A  N/A      3004    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A      5272    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A      5440    C+G   ...eripheral Manager\\DPM.exe    N/A      |\n",
      "|    0   N/A  N/A      7212    C+G   ...ekyb3d8bbwe\\onenoteim.exe    N/A      |\n",
      "|    0   N/A  N/A     10504    C+G   ...1\\jbr\\bin\\jcef_helper.exe    N/A      |\n",
      "|    0   N/A  N/A     11024    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     11068    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     11876    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     12416    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13720    C+G   ...me\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     14944    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     15328    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15720    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
      "|    0   N/A  N/A     16652    C+G   ...4__8wekyb3d8bbwe\\Time.exe    N/A      |\n",
      "|    0   N/A  N/A     18364    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     19276    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A     20840    C+G   ...4__htrsf667h5kn2\\AWCC.exe    N/A      |\n",
      "|    0   N/A  N/A     20996    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     23192    C+G   ...kyb3d8bbwe\\Calculator.exe    N/A      |\n",
      "|    0   N/A  N/A     24060      C   ...da3\\envs\\tf_rs\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     24916    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import rasterio.warp\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "\n",
    "from utils import dataset_gen\n",
    "from utils import metrics_utils\n",
    "\n",
    "# check that a GPU is enabled\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "script_start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the train, validation, and test dataframes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define dictionary with filepaths\n",
    "base_dir = r\"C:\\Users\\ernes\\OneDrive - The Cooper Union for the Advancement of Science and Art\\Thesis-DESKTOP-UID4081\\flood_mapping\\10m_resolution\"\n",
    "\n",
    "train_val_test_pths = {'train_fn_df' : f\"{base_dir}\\\\ds_train_split_10m.csv\",\n",
    "                       'val_fn_df' : f\"{base_dir}\\\\ds_val_split_10m.csv\",\n",
    "                       'test_fn_df' : f\"{base_dir}\\\\ds_test_split_10m.csv\"}\n",
    "\n",
    "train_val_fn_df, test_fn_df, train_size, val_size, test_size =\\\n",
    "    dataset_gen.unet_load_ds_df(train_val_test_pths['train_fn_df'],\n",
    "                                train_val_test_pths['val_fn_df'],\n",
    "                                train_val_test_pths['test_fn_df'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define dictionaries to hold the datasets - the keys will be the different scenarios\n",
    "X_train_dict = {}\n",
    "Y_train_dict = {}\n",
    "\n",
    "X_val_dict = {}\n",
    "Y_val_dict = {}\n",
    "\n",
    "X_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "Y_pred_dict = {}\n",
    "\n",
    "# Define scenario number to scenario name mapping\n",
    "scenario_dict = {1: 'co_event_intensity_only',\n",
    "                 2: 'pre_co_event_intensity',\n",
    "                 3: 'pre_co_event_int_coh'}\n",
    "\n",
    "scenario_num_bands = {1: 2,\n",
    "                      2: 4,\n",
    "                      3: 6}\n",
    "\n",
    "# Define the number of bands per scenario\n",
    "num_bands_dict = {'co_event_intensity_only': 2,\n",
    "                 'pre_co_event_intensity': 4,\n",
    "                 'pre_co_event_int_coh': 6}\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "# define dictionaries to hold the datasets\n",
    "train_val_samples_dict = {}\n",
    "test_samples_dict = {}\n",
    "\n",
    "scenarios = [1, 2, 3]\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    # Create the samples list given the dataframes with file paths as input\n",
    "    train_val_samples_dict[f\"scenario_{scenario}\"], test_samples_dict[f\"scenario_{scenario}\"] = \\\n",
    "        dataset_gen.create_samples_list({'scenario': scenario_dict[scenario],\n",
    "                                            'test_df': test_fn_df,\n",
    "                                            'train_val_df': train_val_fn_df})\n",
    "\n",
    "    # Create data sets dictionary\n",
    "    X_train_dict[f\"scenario_{scenario}\"], X_val_dict[f\"scenario_{scenario}\"], X_test_dict[f\"scenario_{scenario}\"] =\\\n",
    "        dataset_gen.unet_ds_creation({'train_val_list': train_val_samples_dict[f\"scenario_{scenario}\"],\n",
    "                                      'test_list': test_samples_dict[f\"scenario_{scenario}\"]})\n",
    "\n",
    "    # Batch the tensorflow train, val, and test data set generators\n",
    "    X_train_dict[f\"scenario_{scenario}\"] = X_train_dict[f\"scenario_{scenario}\"].batch(10).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    X_val_dict[f\"scenario_{scenario}\"] = X_val_dict[f\"scenario_{scenario}\"].batch(10).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    X_test_dict[f\"scenario_{scenario}\"] = X_test_dict[f\"scenario_{scenario}\"].batch(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Trying out the attention U-net from this repo:\n",
    "\n",
    "https://github.com/yingkaisha/keras-unet-collection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from keras_unet_collection import models\n",
    "# create dictionary to hold the models by scenario\n",
    "unet_2d_models = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 1\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n",
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 2\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n",
      "\n",
      "*******************************************\n",
      "\n",
      "Generating model for scenario: 3\n",
      "Automated hyper-parameter determination is applied with the following details:\n",
      "----------\n",
      "\tNumber of RSU output channels within downsampling blocks: filter_num_down = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within downsampling blocks: filter_mid_num_down = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU output channels within upsampling blocks: filter_num_up = [64, 128, 256, 512, 1024]\n",
      "\tNumber of RSU intermediate channels within upsampling blocks: filter_mid_num_up = [16, 32, 64, 128, 256]\n",
      "\tNumber of RSU-4F output channels within downsampling and bottom blocks: filter_4f_num = [1024, 1024]\n",
      "\tNumber of RSU-4F intermediate channels within downsampling and bottom blocks: filter_4f_num = [512, 512]\n",
      "----------\n",
      "Explicitly specifying keywords listed above if their \"auto\" settings do not satisfy your needs\n",
      "----------\n",
      "The depth of u2net_2d = len(filter_num_down) + len(filter_4f_num) = 7\n",
      "*******************************************\n"
     ]
    }
   ],
   "source": [
    "# loop through scenarios and generate the models\n",
    "for scenario in scenarios:\n",
    "    # Create models for each scenario\n",
    "    print(\"\\n*******************************************\\n\")\n",
    "    print(f\"Generating model for scenario: {scenario}\")\n",
    "    unet_2d_models[f\"scenario_{scenario}\"] = models.u2net_2d((IMG_SIZE, IMG_SIZE, scenario_num_bands[scenario]),\n",
    "                                                             n_labels=2,\n",
    "                                                             filter_num_down=[64, 128, 256, 512, 1024],\n",
    "                                                             filter_num_up='auto',\n",
    "                                                             filter_mid_num_down='auto',\n",
    "                                                             filter_mid_num_up='auto',\n",
    "                                                             filter_4f_num='auto',\n",
    "                                                             filter_4f_mid_num='auto',\n",
    "                                                             activation='ReLU',\n",
    "                                                             output_activation='Sigmoid',\n",
    "                                                             batch_norm=False,\n",
    "                                                             pool=True,\n",
    "                                                             unpool=True,\n",
    "                                                             deep_supervision=False,\n",
    "                                                             name='u2net')\n",
    "    print(\"*******************************************\")\n",
    "\n",
    "#unet_2d_models['scenario_3'].summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.0001,\n",
    "                                                             decay_steps=200,\n",
    "                                                             decay_rate=0.96,\n",
    "                                                             staircase=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     epsilon=1e-07,\n",
    "                                     amsgrad=False,\n",
    "                                     name='Adam')\n",
    "\n",
    "unet_2d_train_hist = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 1 - Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 1\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_model_pth = r\"D:\\SAR_data\\Models\\refactored\\10m_res\\unet_2d\"\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 2 - Pre-event and Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 2\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights for scenario 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 3 - Pre-event and Co-event Intensity Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 3\n",
    "unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the model weights for scenario 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}