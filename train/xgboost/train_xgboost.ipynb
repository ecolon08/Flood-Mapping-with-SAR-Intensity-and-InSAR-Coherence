{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Improving Semantic Water Segmentation by Fusing Sentinel-1 Intensity and Interferometric Synthetic Aperture Radar (InSAR) Coherence Data\n",
    "\n",
    "**Author: Ernesto Colon**\n",
    "**The Cooper Union for the Advancement of Science and Art**\n",
    "\n",
    "#### XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import dataset_gen\n",
    "import xgboost as xgb\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define function to train the XGBoost models in two steps or batches. The data set is large (~28GB for scenario 3) and\n",
    "does not fit in GPU memory. Depending on the GPU memory size, the training pipeline may require training in more than\n",
    "two stages."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def xgb_batch_train(X_train, Y_train, save_fname):\n",
    "    \"\"\"\n",
    "    Function to serialize the XGBoost training for large data sets. This function only handles two batches\n",
    "    since the data set we're using can be split in half and fit in the RTX 3090's memory.\n",
    "\n",
    "    :param X_train: 2D-ndarray with shape (num_pix, num_feat) with input features\n",
    "    :param Y_train: 2D-ndarray with shape (num_pix,) with labels\n",
    "    :param save_fname: string with path and filename to save the final model\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # create first model instance\n",
    "    model_1 = xgb.XGBClassifier(use_label_encoder=False, tree_method='gpu_hist')\n",
    "\n",
    "\n",
    "    # fit first model\n",
    "    model_1.fit(X_train['batch_1'], Y_train['batch_1'])\n",
    "\n",
    "    # create second model instance\n",
    "    model_2 = xgb.XGBClassifier(use_label_encoder=False, tree_method='gpu_hist')\n",
    "\n",
    "    # fit second model\n",
    "    model_2.fit(X_train['batch_2'], Y_train['batch_2'], xgb_model=model_1)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    # Save model\n",
    "    model_2.save_model(save_fname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load previously saved dataset splits**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define dictionary with filepaths\n",
    "base_dir = \"base_dir_path\"\n",
    "\n",
    "train_val_test_pths = {'train_fn_df' : f\"{base_dir}\\\\train_fn_df_fname\",\n",
    "                       'val_fn_df' : f\"{base_dir}\\\\val_fn_df_fname\",\n",
    "                       'test_fn_df' : f\"{base_dir}\\\\test_fn_df_fname\"}\n",
    "\n",
    "train_samples, val_samples, test_samples, train_size, val_size, test_size =\\\n",
    "    dataset_gen.xgboost_load_ds_samples(train_val_test_pths['train_fn_df'],\n",
    "                                train_val_test_pths['val_fn_df'],\n",
    "                                train_val_test_pths['test_fn_df'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dictionaries to store the training and test data sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batches = ['batch_1', 'batch_2']\n",
    "scenarios = ['scenario_1', 'scenario_2', 'scenario_3']\n",
    "\n",
    "X_train_dict = {scenario : {} for scenario in scenarios}\n",
    "Y_train_dict = {scenario : {} for scenario in scenarios}\n",
    "\n",
    "X_test_dict = {scenario : {} for scenario in scenarios}\n",
    "Y_test_dict = {scenario : {} for scenario in scenarios}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the training data set into batches for sequential training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_split_idx_low = [0, int(len(train_samples) / 2)]\n",
    "train_split_idx_high = [int(len(train_samples) / 2), len(train_samples)]\n",
    "\n",
    "test_split_idx_low = [0, int(len(test_samples) / 2)]\n",
    "test_split_idx_high = [int(len(test_samples) / 2), len(test_samples)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 1 Training Pipeline\n",
    "\n",
    "**Co-event intensity data model**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_scenario = 1\n",
    "\n",
    "# Generate data sets for the current scenario\n",
    "for idx, batch in enumerate(batches):\n",
    "    X_train_dict[f\"scenario_{current_scenario}\"][batch], Y_train_dict[f\"scenario_{current_scenario}\"][batch],_ , _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(train_samples[train_split_idx_low[idx] : train_split_idx_high[idx]],\n",
    "                                        coh_flag=False,\n",
    "                                        int_flag=True)\n",
    "\n",
    "    X_test_dict[f\"scenario_{current_scenario}\"], Y_test_dict[f\"scenario_{current_scenario}\"], _, _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples[test_split_idx_low[idx] : test_split_idx_high[idx]],\n",
    "                                        coh_flag=False,\n",
    "                                        int_flag=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**XGBoost training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define path and file name to save the trained model\n",
    "xgboost_model_pth = \"xgboost_model_pth\"\n",
    "fname = f\"{xgboost_model_pth}\\\\scenario_{current_scenario}\\\\xgb_10m_raw_pix_feat_scen_{current_scenario}.model\"\n",
    "\n",
    "# start the stage-wise training pipeline\n",
    "xgb_batch_train(X_train_dict[f\"scenario_{current_scenario}\"], Y_train_dict[f\"scenario_{current_scenario}\"], fname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 2 Training Pipeline\n",
    "\n",
    "**Pre- and Co-event intensity data model**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_scenario = 2\n",
    "\n",
    "# Generate data sets for the current scenario\n",
    "for idx, batch in enumerate(batches):\n",
    "    X_train_dict[f\"scenario_{current_scenario}\"][batch], Y_train_dict[f\"scenario_{current_scenario}\"][batch],_ , _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(train_samples[train_split_idx_low[idx] : train_split_idx_high[idx]],\n",
    "                                        coh_flag=False)\n",
    "\n",
    "    X_test_dict[f\"scenario_{current_scenario}\"], Y_test_dict[f\"scenario_{current_scenario}\"], _, _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples[test_split_idx_low[idx] : test_split_idx_high[idx]],\n",
    "                                        coh_flag=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**XGBoost training**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define path and file name to save the trained model\n",
    "xgboost_model_pth = \"xgboost_model_pth\"\n",
    "fname = f\"{xgboost_model_pth}\\\\scenario_{current_scenario}\\\\xgb_10m_raw_pix_feat_scen_{current_scenario}.model\"\n",
    "\n",
    "# start the stage-wise training pipeline\n",
    "xgb_batch_train(X_train_dict[f\"scenario_{current_scenario}\"], Y_train_dict[f\"scenario_{current_scenario}\"], fname)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scenario 3 Training Pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_scenario = 3\n",
    "\n",
    "# Generate data sets for the current scenario\n",
    "for idx, batch in enumerate(batches):\n",
    "    X_train_dict[f\"scenario_{current_scenario}\"][batch], Y_train_dict[f\"scenario_{current_scenario}\"][batch],_ , _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(train_samples[train_split_idx_low[idx] : train_split_idx_high[idx]],\n",
    "                                        coh_flag=True)\n",
    "\n",
    "    X_test_dict[f\"scenario_{current_scenario}\"], Y_test_dict[f\"scenario_{current_scenario}\"], _, _ = \\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples[test_split_idx_low[idx] : test_split_idx_high[idx]],\n",
    "                                        coh_flag=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**XGBoost training**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define path and file name to save the trained model\n",
    "xgboost_model_pth = \"xgboost_model_pth\"\n",
    "fname = f\"{xgboost_model_pth}\\\\scenario_{current_scenario}\\\\xgb_10m_raw_pix_feat_scen_{current_scenario}.model\"\n",
    "\n",
    "# start the stage-wise training pipeline\n",
    "xgb_batch_train(X_train_dict[f\"scenario_{current_scenario}\"], Y_train_dict[f\"scenario_{current_scenario}\"], fname)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}