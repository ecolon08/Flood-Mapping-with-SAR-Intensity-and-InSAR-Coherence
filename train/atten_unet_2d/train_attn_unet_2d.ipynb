{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Improving Semantic Water Segmentation by Fusing Sentinel-1 Intensity and Interferometric Synthetic Aperture Radar\n",
    "### (InSAR) Coherence Data\n",
    "\n",
    "**Author: Ernesto Colon**\n",
    "**The Cooper Union for the Advancement of Science and Art**\n",
    "\n",
    "#### Attention Unet-2D Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from utils import dataset_gen\n",
    "\n",
    "# check that a GPU is enabled\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "script_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load the train, validation, and test dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionary with filepaths\n",
    "base_dir = \"base_dir_path\"\n",
    "\n",
    "train_val_test_pths = {'train_fn_df' : f\"{base_dir}\\\\ds_train_split_10m.csv\",\n",
    "                       'val_fn_df' : f\"{base_dir}\\\\ds_val_split_10m.csv\",\n",
    "                       'test_fn_df' : f\"{base_dir}\\\\ds_test_split_10m.csv\"}\n",
    "\n",
    "train_val_fn_df, test_fn_df, train_size, val_size, test_size =\\\n",
    "    dataset_gen.unet_load_ds_df(train_val_test_pths['train_fn_df'],\n",
    "                                train_val_test_pths['val_fn_df'],\n",
    "                                train_val_test_pths['test_fn_df'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We generate datasets for the following scenarios:\n",
    "\n",
    "- Scenario 1: Co-event intensity data only\n",
    "- Scenario 2: Pre- and co-event intensity data only\n",
    "- Scenario 3: Pre- and co-event intensity and coherence data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionaries to hold the datasets - the keys will be the different scenarios\n",
    "X_train_dict = {}\n",
    "Y_train_dict = {}\n",
    "\n",
    "X_val_dict = {}\n",
    "Y_val_dict = {}\n",
    "\n",
    "X_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "Y_pred_dict = {}\n",
    "\n",
    "# Define scenario number to scenario name mapping\n",
    "scenario_dict = {1: 'co_event_intensity_only',\n",
    "                 2: 'pre_co_event_intensity',\n",
    "                 3: 'pre_co_event_int_coh'}\n",
    "\n",
    "scenario_num_bands = {1: 2,\n",
    "                      2: 4,\n",
    "                      3: 6}\n",
    "\n",
    "# Define the number of bands per scenario\n",
    "num_bands_dict = {'co_event_intensity_only': 2,\n",
    "                 'pre_co_event_intensity': 4,\n",
    "                 'pre_co_event_int_coh': 6}\n",
    "\n",
    "IMG_SIZE = 512\n",
    "\n",
    "# define dictionaries to hold the datasets\n",
    "train_val_samples_dict = {}\n",
    "test_samples_dict = {}\n",
    "\n",
    "# Loop through each scenario and create the tensorflow data loaders\n",
    "scenarios = [1, 2, 3]\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    # Create the samples list given the dataframes with file paths as input\n",
    "    train_val_samples_dict[f\"scenario_{scenario}\"], test_samples_dict[f\"scenario_{scenario}\"] = \\\n",
    "        dataset_gen.create_samples_list({'scenario': scenario_dict[scenario],\n",
    "                                            'test_df': test_fn_df,\n",
    "                                            'train_val_df': train_val_fn_df})\n",
    "\n",
    "    # Create data sets dictionary\n",
    "    X_train_dict[f\"scenario_{scenario}\"], X_val_dict[f\"scenario_{scenario}\"], X_test_dict[f\"scenario_{scenario}\"] =\\\n",
    "        dataset_gen.unet_ds_creation({'train_val_list': train_val_samples_dict[f\"scenario_{scenario}\"],\n",
    "                                      'test_list': test_samples_dict[f\"scenario_{scenario}\"]})\n",
    "\n",
    "    # Batch the tensorflow train, val, and test data set generators\n",
    "    X_train_dict[f\"scenario_{scenario}\"] =\\\n",
    "        X_train_dict[f\"scenario_{scenario}\"].batch(10).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    X_val_dict[f\"scenario_{scenario}\"] =\\\n",
    "        X_val_dict[f\"scenario_{scenario}\"].batch(10).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    X_test_dict[f\"scenario_{scenario}\"] = X_test_dict[f\"scenario_{scenario}\"].batch(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention U-Net Models\n",
    "\n",
    "For this study, we leverage the publicly available Keras UNet Collection linked below.\n",
    "\n",
    "https://github.com/yingkaisha/keras-unet-collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras_unet_collection import models\n",
    "\n",
    "# create dictionary to hold the models by scenario\n",
    "attn_unet_2d_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# loop through scenarios and generate the models\n",
    "for scenario in scenarios:\n",
    "    # Create models for each scenario\n",
    "    print(\"\\n*******************************************\\n\")\n",
    "    print(f\"Generating model for scenario: {scenario}\")\n",
    "\n",
    "    attn_unet_2d_models[f\"scenario_{scenario}\"] = models.att_unet_2d((IMG_SIZE, IMG_SIZE, scenario_num_bands[scenario]),\n",
    "                                                                filter_num=[64, 128, 256, 512, 1024],\n",
    "                                                                n_labels=2,\n",
    "                                                                stack_num_down=2,\n",
    "                                                                stack_num_up=2,\n",
    "                                                                activation='ReLU',\n",
    "                                                                atten_activation='ReLU',\n",
    "                                                                attention='add',\n",
    "                                                                output_activation='Sigmoid',\n",
    "                                                                batch_norm=True,\n",
    "                                                                pool=False,\n",
    "                                                                unpool=False,\n",
    "                                                                backbone='VGG16',\n",
    "                                                                weights=None,\n",
    "                                                                freeze_backbone=False,\n",
    "                                                                freeze_batch_norm=True,\n",
    "                                                                name='attunet')\n",
    "\n",
    "\n",
    "    print(\"*******************************************\")\n",
    "\n",
    "#unet_2d_models['scenario_3'].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(0.0001,\n",
    "                                                             decay_steps=200,\n",
    "                                                             decay_rate=0.96,\n",
    "                                                             staircase=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.999,\n",
    "                                     epsilon=1e-07,\n",
    "                                     amsgrad=False,\n",
    "                                     name='Adam')\n",
    "\n",
    "# Create dictionary to store model training history\n",
    "attn_unet_2d_train_hist = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 Training- Co-event Intensity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 1\n",
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    attn_unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights for scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attn_unet_2d_model_pth = \"atten_unet_model_path\"\n",
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{attn_unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot training and validation loss for scenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_train_val_loss(model_history):\n",
    "    loss = model_history.history['loss']\n",
    "    val_loss = model_history.history['val_loss']\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "    plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss Value')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val_loss(attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2 Training - Pre-event and Co-event Intensity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 2\n",
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 30\n",
    "attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    attn_unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights for scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{attn_unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Plot training and validation loss for scenario 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val_loss(attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scenario 3 Training - Pre-event and Co-event Intensity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "current_scenario = 3\n",
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].compile(optimizer=optimizer,\n",
    "                                               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                                               metrics=['accuracy'])\n",
    "\n",
    "# Start training routine\n",
    "train_start_time = time.time()\n",
    "\n",
    "EPOCHS = 1\n",
    "attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"] =\\\n",
    "    attn_unet_2d_models[f\"scenario_{current_scenario}\"].fit(X_train_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               validation_data=X_val_dict[f\"scenario_{current_scenario}\"],\n",
    "                                               epochs=EPOCHS)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - train_start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model weights for scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "attn_unet_2d_models[f\"scenario_{current_scenario}\"].save_weights(\n",
    "    f\"{attn_unet_2d_model_pth}\\\\scenario_{current_scenario}\" + \"\\\\\" + f\"unet2d_10m_{scenario_dict[current_scenario]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training and validation loss for scenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_train_val_loss(attn_unet_2d_train_hist[f\"scenario_{current_scenario}\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}