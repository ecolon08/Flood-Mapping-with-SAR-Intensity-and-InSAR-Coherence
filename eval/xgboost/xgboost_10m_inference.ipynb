{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Improving Semantic Water Segmentation by Fusing Sentinel-1 Intensity and Interferometric Synthetic Aperture Radar\n",
    "### (InSAR) Coherence Data\n",
    "\n",
    "**Author: Ernesto Colon**\n",
    "**The Cooper Union for the Advancement of Science and Art**\n",
    "\n",
    "#### XGBoost Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from utils import metrics_utils\n",
    "from utils import dataset_gen\n",
    "from utils import general_utils\n",
    "import time\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Load previously saved dataset splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionary with filepaths\n",
    "base_dir = \"base_dir\"\n",
    "\n",
    "train_val_test_pths = {'train_fn_df' : f\"{base_dir}\\\\ds_train_split_10m.csv\",\n",
    "                       'val_fn_df' : f\"{base_dir}\\\\ds_val_split_10m.csv\",\n",
    "                       'test_fn_df' : f\"{base_dir}\\\\ds_test_split_10m.csv\"}\n",
    "\n",
    "train_samples, val_samples, test_samples, train_size, val_size, test_size =\\\n",
    "    dataset_gen.xgboost_load_ds_samples(train_val_test_pths['train_fn_df'],\n",
    "                                train_val_test_pths['val_fn_df'],\n",
    "                                train_val_test_pths['test_fn_df'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define category names and a color mapping for semantic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define category names\n",
    "tgt_cat_names = {\n",
    "    0: 'Not water',\n",
    "    1: 'Water'\n",
    "}\n",
    "\n",
    "# Define the colors per category\n",
    "wtr_clrs_hex = ['#f7f7f7', '#67a9cf']\n",
    "\n",
    "# Generate the labels colormap\n",
    "wtr_cmap = general_utils.gen_cmap(wtr_clrs_hex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate data sets for inference\n",
    "\n",
    "We generate datasets for the following scenarios:\n",
    "\n",
    "- Scenario 1: Co-event intensity data only\n",
    "- Scenario 2: Pre- and co-event intensity data only\n",
    "- Scenario 3: Pre- and co-event intensity and coherence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define dictionaries to hold the datasets - the keys will be the different scenarios\n",
    "X_train_dict = {}\n",
    "Y_train_dict = {}\n",
    "\n",
    "X_test_dict = {}\n",
    "Y_test_dict = {}\n",
    "\n",
    "Y_pred_dict = {}\n",
    "\n",
    "scenarios = ['scenario_1', 'scenario_2', 'scenario_3']\n",
    "\n",
    "# Loop through each scenario and generate / load the data sets to memory\n",
    "for scenario in scenarios:\n",
    "    # logic to determine whether the current scenario includes coherence data or not\n",
    "    if scenario == 'scenario_1':\n",
    "        int_flag = True\n",
    "    else:\n",
    "        int_flag = False\n",
    "    if scenario == 'scenario_3':\n",
    "        coh_flag = True\n",
    "    else:\n",
    "        coh_flag = False\n",
    "\n",
    "    # generate data set\n",
    "    X_test_dict[scenario], Y_test_dict[scenario], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples, coh_flag=coh_flag, int_flag=int_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Gather dataset parameters we'll need later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_train_samp = len(train_samples)\n",
    "img_size = 512\n",
    "\n",
    "num_feat_dict = {'scenario_1': 2,\n",
    "                 'scenario_2': 4,\n",
    "                 'scenario_3': 6,\n",
    "                 'scenario_1_hand_lbl': 2,\n",
    "                 'scenario_2_hand_lbl': 4,\n",
    "                 'scenario_3_hand_lbl': 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Hand Labeled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load hand label dataset\n",
    "hand_lbl_ds_pth = \"hand_lbl_ds_pth\"\n",
    "hand_lbl_ds_fname = f\"{hand_lbl_ds_pth}hand_lbl_ds_10m_res.csv\"\n",
    "\n",
    "# load csv file to dataframe\n",
    "df_hand_lbl_samples = pd.read_csv(hand_lbl_ds_fname)\n",
    "\n",
    "# loop through df and append sample paths to a list\n",
    "hand_lbl_samples = list()\n",
    "\n",
    "for idx, row in df_hand_lbl_samples.iterrows():\n",
    "    hand_lbl_samples.append((row['s1'],\n",
    "                             row['pre_event_grd'],\n",
    "                             row['pre_event_coh'],\n",
    "                             row['co_event_coh'],\n",
    "                             row['hand_lbl']))\n",
    "\n",
    "hand_lbl_scenarios = [f\"{scenario}_hand_lbl\" for scenario in scenarios]\n",
    "\n",
    "# Generate hand-labeled data set\n",
    "for scenario in hand_lbl_scenarios:\n",
    "    # logic to determine whether the current scenario includes coherence data or not\n",
    "    if scenario == 'scenario_1_hand_lbl':\n",
    "        int_flag = True\n",
    "    else:\n",
    "        int_flag = False\n",
    "    if scenario == 'scenario_3_hand_lbl':\n",
    "        coh_flag = True\n",
    "    else:\n",
    "        coh_flag = False\n",
    "\n",
    "    X_test_dict[scenario], Y_test_dict[scenario], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(hand_lbl_samples, coh_flag=coh_flag, int_flag=int_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Visualize some image-target pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load a number of scenes\n",
    "scenes_list = list()\n",
    "\n",
    "num_scenes = 5\n",
    "\n",
    "for idx in range(num_scenes):\n",
    "    temp_list = list()\n",
    "\n",
    "    for j in range(len(train_samples[idx])):\n",
    "\n",
    "        # Open rasters with rasterio\n",
    "        with rasterio.open(train_samples[idx][j]) as src:\n",
    "            src = src.read()\n",
    "            if j == 4:      # account for labels and map the not-valid pixels to the not-water category\n",
    "                src = np.where(src == -1, 0, src)\n",
    "            temp_list.append(src)\n",
    "\n",
    "    scenes_list.append(temp_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Display the scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_col = 7\n",
    "\n",
    "fig, ax = plt.subplots(num_scenes, num_col, figsize=(80, num_scenes * 10))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i in range(len(scenes_list)):\n",
    "    # s1 co-event\n",
    "    ax[num_col*i].imshow(scenes_list[i][0][0, :, :], cmap='gray')\n",
    "    ax[num_col*i].set_title('S1 co-event VH')\n",
    "\n",
    "    ax[num_col*i + 1].imshow(scenes_list[i][0][1, :, :], cmap='gray')\n",
    "    ax[num_col*i + 1].set_title('S1 co-event VV')\n",
    "\n",
    "    # s1 pre-event\n",
    "    ax[num_col*i + 2].imshow(scenes_list[i][1][0, :, :], cmap='gray')\n",
    "    ax[num_col*i + 2].set_title('S1 pre-event VH')\n",
    "\n",
    "    ax[num_col*i + 3].imshow(scenes_list[i][1][1, :, :], cmap='gray')\n",
    "    ax[num_col*i + 3].set_title('S1 pre-event VV')\n",
    "\n",
    "    # pre-event coh\n",
    "    ax[num_col*i + 4].imshow(scenes_list[i][2][0, :, :], cmap='gray')\n",
    "    ax[num_col*i + 4].set_title('Pre-event coherence')\n",
    "\n",
    "    # co-event coh\n",
    "    ax[num_col*i + 5].imshow(scenes_list[i][3][0, :, :], cmap='gray')\n",
    "    ax[num_col*i + 5].set_title('Co-event coherence')\n",
    "\n",
    "    # s2 label\n",
    "    ax[num_col*i + 6].imshow(scenes_list[i][4][0, :, :], cmap=wtr_cmap)\n",
    "    ax[num_col*i + 6].set_title('S2 Label')\n",
    "\n",
    "for ax in ax:\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks([]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### XGBoost Models\n",
    "\n",
    "Load previously trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "xgb_models_dir = {'scenario_1': \"model_scen_1_pth\",\n",
    "                  'scenario_2': \"model_scen_2_pth\",\n",
    "                  'scenario_3': \"model_scen_3_pth\"}\n",
    "\n",
    "xgb_classifier_models = {}\n",
    "\n",
    "for scenario in xgb_models_dir.keys():\n",
    "\n",
    "    xgb_classifier_models[scenario] = xgb.XGBClassifier(use_label_encoder=False,\n",
    "                                                        tree_method='gpu_hist')\n",
    "\n",
    "    print(f\"Loading model weights for scenario: {scenario}...\")\n",
    "    xgb_classifier_models[scenario].load_model(xgb_models_dir[scenario])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make predictions with the XGBoost models\n",
    "\n",
    "**Notes**\n",
    "\n",
    "The held-out test set is comprised of Sentinel-2 weak labels from the Sen1Floods11 data set.\n",
    "\n",
    "The hand-labeled data set is also provided by the Sen1Floods11 data set, and provides an independent data set not\n",
    "used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# predict on the held-out test dataset\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# loop through each scenario\n",
    "for scenario in xgb_classifier_models.keys():\n",
    "\n",
    "    Y_pred_dict[scenario] = xgb_classifier_models[scenario].predict(X_test_dict[scenario])\n",
    "\n",
    "# Predict on the hand-labeled test dataset\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    Y_pred_dict[f\"{scenario}_hand_lbl\"] = xgb_classifier_models[scenario].predict(X_test_dict[f\"{scenario}_hand_lbl\"])\n",
    "\n",
    "print(f\"Inference took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compute Metrics\n",
    "\n",
    "For metrics, we compute:\n",
    "\n",
    "- Overall accuracy\n",
    "- Mean intersection over union, mIoU\n",
    "- Jaccard score\n",
    "- Water precision\n",
    "- Water recall\n",
    "- Water f1-score\n",
    "- Not-Water precision\n",
    "- Not-Water recall\n",
    "- Not-Water f1-score\n",
    "-\n",
    "#### Held-out Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "summary_df = metrics_utils.summary_report(Y_test_dict, Y_pred_dict)\n",
    "\n",
    "print(f\"Process took: {time.time() - start_time} seconds\")\n",
    "\n",
    "# save summary to csv file\n",
    "xgboost_summ_pth = \"xgboost_summ_pth\"\n",
    "fname = \"xgboost_summary_stats.csv\"\n",
    "summary_df.to_csv(f\"{xgboost_summ_pth}\\\\{fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing IoU per class (i.e., water and not-water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "miou_per_class = metrics_utils.miou_per_class(Y_test_dict, Y_pred_dict)\n",
    "\n",
    "# save to csv file\n",
    "mIou_fname = \"xgboost_10m_mIoU_per_class_stats.csv\"\n",
    "miou_per_class.to_csv(f\"{xgboost_summ_pth}\\\\{mIou_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing Models Ability to Generalize\n",
    "\n",
    "We use data over the Sri-Lanka region (both weakly labeled as well as hand-labeled) to test the models' ability\n",
    "to generalize\n",
    "\n",
    "#### Generate generalization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a list with all regions for both the held-out test set and the hand-labeled test set\n",
    "regions = ['USA', 'Mekong', 'Colombia', 'Paraguay', 'India', 'Bolivia']\n",
    "regions_w_hand_lbl = [region for region in regions if region != \"Colombia\"]\n",
    "\n",
    "generalization_ds_pth = \"generalization_ds_pth\"\n",
    "\n",
    "# Create empty list to store the samples' path\n",
    "gen_test_samples = []\n",
    "\n",
    "# Grab the number of samples in the data set\n",
    "gen_test_fn_df = pd.read_csv(generalization_ds_pth)\n",
    "\n",
    "\n",
    "for idx, row in gen_test_fn_df.iterrows():\n",
    "    gen_test_samples.append((row['s1'], row['pre_event_grd'], row['pre_event_coh'], row['co_event_coh'], row['s2_lbl']))\n",
    "\n",
    "num_gen_samp = len(gen_test_samples)\n",
    "\n",
    "# create dictionaries to store the data sets\n",
    "gener_X_test_dict = dict()\n",
    "gener_Y_test_dict = dict()\n",
    "\n",
    "# Loop through each scenario\n",
    "for scenario in scenarios:\n",
    "    # logic to determine whether the current scenario includes coherence data or not\n",
    "    if scenario == 'scenario_1':\n",
    "        int_flag = True\n",
    "    else:\n",
    "        int_flag = False\n",
    "    if scenario == 'scenario_3':\n",
    "        coh_flag = True\n",
    "    else:\n",
    "        coh_flag = False\n",
    "\n",
    "    gener_X_test_dict[scenario], gener_Y_test_dict[scenario], _, _ = dataset_gen.rf_xgb_ds_generator(gen_test_samples, coh_flag=coh_flag, int_flag=int_flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Hand-Labeled Generalization Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load hand label dataset\n",
    "gen_hand_lbl_ds_pth = \"gen_hand_lbl_ds_pth\"\n",
    "\n",
    "# read hand-labeled data set into dataframe\n",
    "gen_df_hand_lbl_samples = pd.read_csv(gen_hand_lbl_ds_pth)\n",
    "\n",
    "# create dict to store the data set\n",
    "gen_hand_samples_by_region_dict = {}\n",
    "\n",
    "# For now, we only have Sri-Lanka as the generalization region\n",
    "regions = ['Sri-Lanka']\n",
    "\n",
    "for region in regions:\n",
    "    # temp list to store file paths\n",
    "    pths = list()\n",
    "\n",
    "    # pluck the test sample paths by region\n",
    "    test_pth_region = gen_df_hand_lbl_samples[gen_df_hand_lbl_samples.s1.str.contains(region)]\n",
    "\n",
    "    for idx, row in test_pth_region.iterrows():\n",
    "        pths.append((row['s1'], row['pre_event_grd'], row['pre_event_coh'], row['co_event_coh'], row['hand_lbl']))\n",
    "\n",
    "    gen_hand_samples_by_region_dict[region] = pths\n",
    "\n",
    "# Generate hand-labeled generalization test dataset\n",
    "\n",
    "for scenario in hand_lbl_scenarios:\n",
    "    # logic to determine whether the current scenario includes coherence data or not\n",
    "    if scenario == 'scenario_1_hand_lbl':\n",
    "        int_flag = True\n",
    "    else:\n",
    "        int_flag = False\n",
    "    if scenario == 'scenario_3_hand_lbl':\n",
    "        coh_flag = True\n",
    "    else:\n",
    "        coh_flag = False\n",
    "\n",
    "    gener_X_test_dict[scenario], gener_Y_test_dict[scenario], _, _ = dataset_gen.rf_xgb_ds_generator(gen_hand_samples_by_region_dict['Sri-Lanka'], coh_flag=coh_flag, int_flag=int_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Make Predictions on the generalization data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "gener_Y_pred_dict = dict()\n",
    "\n",
    "for scenario in xgb_classifier_models.keys():\n",
    "\n",
    "    gener_Y_pred_dict[scenario] = xgb_classifier_models[scenario].predict(gener_X_test_dict[scenario])\n",
    "\n",
    "# Predict on the hand-labeled test dataset\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    gener_Y_pred_dict[f\"{scenario}_hand_lbl\"] = xgb_classifier_models[scenario].predict(gener_X_test_dict[f\"{scenario}_hand_lbl\"])\n",
    "\n",
    "print(f\"Inference took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compute metrics for generalization data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "gener_summary_df = metrics_utils.summary_report(gener_Y_test_dict, gener_Y_pred_dict)\n",
    "\n",
    "print(f\"Process took: {time.time() - start_time} seconds\")\n",
    "\n",
    "# save the metrics to a csv file for later recall\n",
    "gener_summ_fname = \"xgboost_10m_generalization_stats.csv\"\n",
    "gener_summary_df.to_csv(f\"{xgboost_summ_pth}\\\\{gener_summ_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute IoU per class for generalization dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gener_miou_per_class = metrics_utils.miou_per_class(gener_Y_test_dict, gener_Y_pred_dict)\n",
    "\n",
    "# save to csv\n",
    "gener_miou_fname = \"xgboost_10m_generalization_mIoU_stats.csv\"\n",
    "gener_miou_per_class.to_csv(f\"{xgboost_summ_pth}\\\\{gener_miou_fname}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Making Inferences Aggregated by Geographical Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read csv file with the test filepaths\n",
    "test_fn_df = pd.read_csv(train_val_test_pths['test_fn_df'])\n",
    "\n",
    "test_samples_by_region_dict = {}\n",
    "regions = ['USA', 'Mekong', 'Colombia', 'Paraguay', 'India', 'Bolivia']\n",
    "\n",
    "for region in regions:\n",
    "    pths = list()\n",
    "\n",
    "    # pluck the test sample paths by region\n",
    "    test_pth_region = test_fn_df[test_fn_df.s1.str.contains(region)]\n",
    "\n",
    "    for idx, row in test_pth_region.iterrows():\n",
    "        pths.append((row['s1'], row['pre_event_grd'], row['pre_event_coh'], row['co_event_coh'], row['s2_lbl']))\n",
    "\n",
    "    test_samples_by_region_dict[region] = pths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the data sets per region\n",
    "all_scenarios = scenarios + hand_lbl_scenarios\n",
    "\n",
    "# Create schemas for the data sets\n",
    "X_test_ds_region_dict = {region: {} for region in regions}\n",
    "Y_test_ds_region_dict = {region: {} for region in regions}\n",
    "\n",
    "for region in regions:\n",
    "\n",
    "    # Scenario 1\n",
    "    X_test_ds_region_dict[region]['scenario_1'], Y_test_ds_region_dict[region]['scenario_1'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples_by_region_dict[region], coh_flag=False, int_flag=True)\n",
    "\n",
    "    # Scenario 2\n",
    "    X_test_ds_region_dict[region]['scenario_2'], Y_test_ds_region_dict[region]['scenario_2'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples_by_region_dict[region], coh_flag=False, int_flag=False)\n",
    "\n",
    "    # Scenario 3\n",
    "    X_test_ds_region_dict[region]['scenario_3'], Y_test_ds_region_dict[region]['scenario_3'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(test_samples_by_region_dict[region], coh_flag=True, int_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make inferences by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "Y_pred_region_dict = {region : {} for region in regions}\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    for region in regions:\n",
    "\n",
    "        Y_pred_region_dict[region][scenario] =\\\n",
    "            xgb_classifier_models[scenario].predict(X_test_ds_region_dict[region][scenario])\n",
    "\n",
    "print(f\"Inference took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compute predictions on hand-labeled dataset aggregated by region\n",
    "\n",
    "**Note: Colombia does not have hand-labeled chips**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hand_lbl_samples_region_dict = {}\n",
    "\n",
    "# Colombia does not have any hand labels\n",
    "regions = ['USA', 'Mekong', 'Paraguay', 'India', 'Bolivia']\n",
    "\n",
    "for region in regions:\n",
    "    pths = list()\n",
    "\n",
    "    # pluck the test sample paths by region\n",
    "    test_pth_region = df_hand_lbl_samples[df_hand_lbl_samples.s1.str.contains(region)]\n",
    "\n",
    "    for idx, row in test_pth_region.iterrows():\n",
    "        pths.append((row['s1'], row['pre_event_grd'], row['pre_event_coh'], row['co_event_coh'], row['hand_lbl']))\n",
    "\n",
    "    hand_lbl_samples_region_dict[region] = pths\n",
    "\n",
    "\n",
    "for region in regions:\n",
    "\n",
    "    # Scenario 2\n",
    "    X_test_ds_region_dict[region]['scenario_1_hand_lbl'], Y_test_ds_region_dict[region]['scenario_1_hand_lbl'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(hand_lbl_samples_region_dict[region], coh_flag=False, int_flag=True)\n",
    "\n",
    "    # Scenario 4\n",
    "    X_test_ds_region_dict[region]['scenario_2_hand_lbl'], Y_test_ds_region_dict[region]['scenario_2_hand_lbl'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(hand_lbl_samples_region_dict[region], coh_flag=False, int_flag=False)\n",
    "\n",
    "    # Scenario 5\n",
    "    X_test_ds_region_dict[region]['scenario_3_hand_lbl'], Y_test_ds_region_dict[region]['scenario_3_hand_lbl'], _, _ =\\\n",
    "        dataset_gen.rf_xgb_ds_generator(hand_lbl_samples_region_dict[region], coh_flag=True, int_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Make inferences with the hand-labeled dataset aggregated by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for scenario in scenarios:\n",
    "\n",
    "    for region in regions:\n",
    "\n",
    "        Y_pred_region_dict[region][f'{scenario}_hand_lbl'] =\\\n",
    "            xgb_classifier_models[scenario].predict(X_test_ds_region_dict[region][f'{scenario}_hand_lbl'])\n",
    "\n",
    "print(f\"Inference took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Generate prediction summaries by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "regions = ['USA', 'Mekong', 'Colombia', 'Paraguay', 'India', 'Bolivia']\n",
    "\n",
    "xgboost_summ_pth = \"xgboost_summ_pth\"\n",
    "\n",
    "summary_by_region = {}\n",
    "\n",
    "for region in regions:\n",
    "    print(f\"Region: {region}\\n\\n\")\n",
    "    summary_by_region[region] = metrics_utils.summary_report(Y_test_ds_region_dict[region], Y_pred_region_dict[region])\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # save to csv\n",
    "    summary_by_region[region].to_csv(f\"{xgboost_summ_pth}\\\\{region}_summary_stats.csv\")\n",
    "\n",
    "print(f\"Process took: {time.time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Compute IoU per class aggregated by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create dict to store the IoU metrics by region\n",
    "regional_miou_per_class = {}\n",
    "\n",
    "for region in regions:\n",
    "    print(f\"Region: {region}\\n\\n\")\n",
    "\n",
    "    regional_miou_per_class[region] = metrics_utils.miou_per_class(Y_test_ds_region_dict[region],\n",
    "                                                                   Y_pred_region_dict[region])\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "    # save to csv\n",
    "    regional_miou_per_class[region].to_csv(f\"{xgboost_summ_pth}\\\\{region}_mIoU_stats.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generate labels and label overlap by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the generalization data set with the rest of the data sets\n",
    "Y_pred_region_dict['Sri-Lanka'] = gener_Y_pred_dict\n",
    "\n",
    "Y_test_ds_region_dict['Sri-Lanka'] = gener_Y_test_dict\n",
    "\n",
    "X_test_ds_region_dict['Sri-Lanka'] = gener_X_test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Reshape predictions for visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_regions = list(Y_pred_region_dict.keys())\n",
    "all_regions_hand_lbl = [region for region in all_regions if region != 'Colombia']\n",
    "\n",
    "# Create dicts to store the ground truth, predicted labels and the intensity rasters for visualization\n",
    "Y_pred_hand_lbl_by_region = {}\n",
    "Y_true_hand_lbl_by_region = {}\n",
    "X_test_hand_lbl_by_region = {}\n",
    "\n",
    "# scenarios to pluck\n",
    "scen_to_pluck = ['scenario_1_hand_lbl', 'scenario_2_hand_lbl', 'scenario_3_hand_lbl']\n",
    "\n",
    "# Create schema to store the predictions and test data\n",
    "Y_pred_hand_lbl_by_region = {region : {scen : [] for scen in scen_to_pluck} for region in all_regions_hand_lbl}\n",
    "Y_true_hand_lbl_by_region = {region : {scen : [] for scen in scen_to_pluck} for region in all_regions_hand_lbl}\n",
    "X_test_hand_lbl_by_region = {region : {scen : [] for scen in scen_to_pluck} for region in all_regions_hand_lbl}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Copy the predictions and the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for region in all_regions_hand_lbl:\n",
    "    for scen in scen_to_pluck:\n",
    "        try:\n",
    "            Y_pred_hand_lbl_by_region[region][scen] = Y_pred_region_dict[region][scen].copy()\n",
    "\n",
    "            Y_true_hand_lbl_by_region[region][scen] = Y_test_ds_region_dict[region][scen].copy()\n",
    "\n",
    "            X_test_hand_lbl_by_region[region][scen] = X_test_ds_region_dict[region][scen].copy()\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create function to generate the label overlap between ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "img_size = 512\n",
    "\n",
    "def gen_lbl_overlap(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to return a semantic map with a label overlap given ground truth and predicted labels\n",
    "    :param y_true: ndarray with ground truth labels\n",
    "    :param y_pred: ndarray with predicted labels\n",
    "    :return: combined, an ndarray with 4 classes (1: true positive, 2: true negatives, 3: false positives, 4: false neg)\n",
    "    \"\"\"\n",
    "\n",
    "    # allocate space to store the label overlap\n",
    "    combined = np.zeros(y_pred.shape)\n",
    "\n",
    "    # true positives are labels that are predicted as water (1)\n",
    "    tp =np.logical_and(np.where(y_pred == 1, 1, 0), np.where(y_true == 1, 1, 0))\n",
    "\n",
    "    # true negatives\n",
    "    tn = np.logical_and(np.where(y_pred == 0, 1, 0), np.where(y_true == 0, 1, 0))\n",
    "\n",
    "    # false positives are labels that were labeled as 1 but that were 0 in reality\n",
    "    fp = np.logical_and(np.where(y_pred == 1, 1, 0), np.where(y_true == 0, 1, 0))\n",
    "\n",
    "    # false negatives are labels that were labeled as 0 but were 1 in reality\n",
    "    fn = np.logical_and(np.where(y_pred == 0, 1, 0), np.where(y_true == 1, 1, 0))\n",
    "\n",
    "    # combine all classes\n",
    "    combined[tp] = 1\n",
    "    combined[tn] = 2\n",
    "    combined[fp] = 3\n",
    "    combined[fn] = 4\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compute label overlap by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lbl_ovrlap_by_region = {region : {scen : [] for scen in scen_to_pluck} for region in all_regions_hand_lbl}\n",
    "\n",
    "for region in all_regions_hand_lbl:\n",
    "    for scen in scen_to_pluck:\n",
    "        lbl_ovrlap_by_region[region][scen] =\\\n",
    "            np.reshape(gen_lbl_overlap(\n",
    "                Y_true_hand_lbl_by_region[region][scen],\n",
    "                Y_pred_hand_lbl_by_region[region][scen]),\n",
    "                (-1, img_size, img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Reshape ground truth and display the label overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for region in all_regions_hand_lbl:\n",
    "    print(region)\n",
    "    for scen in scen_to_pluck:\n",
    "        Y_pred_hand_lbl_by_region[region][scen] = np.reshape(Y_pred_hand_lbl_by_region[region][scen], (-1, img_size, img_size))\n",
    "\n",
    "        Y_true_hand_lbl_by_region[region][scen] = np.reshape(Y_true_hand_lbl_by_region[region][scen], (-1, img_size, img_size))\n",
    "\n",
    "        X_test_hand_lbl_by_region[region][scen] = np.reshape(X_test_hand_lbl_by_region[region][scen], (-1, img_size, img_size, num_feat_dict[scen]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to plot the label overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generate color maps for the labels and label overlap\n",
    "\n",
    "from utils import general_utils\n",
    "\n",
    "wtr_cmap = general_utils.gen_cmap(['#f7f7f7', '#67a9cf'])\n",
    "ovrlp_cmap = general_utils.gen_cmap(['#67a9cf', '#f7f7f7', '#ef8a62', '#999999'])\n",
    "\n",
    "from mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\n",
    "import matplotlib.font_manager as fm\n",
    "fontprops = fm.FontProperties(size=15)\n",
    "\n",
    "def display_lbl_overlap(y_true, lbl_overlap, x_test, num_plot, region, indices=None):\n",
    "    \"\"\"\n",
    "    Function to display the label overlap\n",
    "    :param y_true: ndarray with ground truth labels\n",
    "    :param lbl_overlap: ndarray with label overlap\n",
    "    :param x_test: ndarray with Sentinel-1 co-event intensity (VH) raster\n",
    "    :param num_plot: integer, number of scenes to display\n",
    "    :param region: string with geographical region to display\n",
    "    :param indices: list of integer with indices to plot from the entire data set\n",
    "    :return: matplotlib figure handle\n",
    "    \"\"\"\n",
    "\n",
    "    fontprops = fm.FontProperties(size=12)\n",
    "\n",
    "    num_col = 5\n",
    "    fig, ax = plt.subplots(num_plot + 1, num_col, figsize=(20, 5 * num_plot))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    if indices == None:\n",
    "        indices = range(num_plot)\n",
    "\n",
    "    for idx, raster in enumerate(indices):\n",
    "\n",
    "        ax[num_col * idx].imshow(x_test[region]['scenario_1_hand_lbl'][raster, :, :, 0], cmap='gray')\n",
    "        ax[num_col * idx].set_title(f'Co-event Intensity (VH)')\n",
    "\n",
    "        # plot ground truth\n",
    "        ax[num_col * idx + 1].imshow(y_true[region]['scenario_3_hand_lbl'][raster, :, :], cmap=wtr_cmap)\n",
    "        #ax[num_col * idx + 1].set_title(f'Ground Truth Label, index: {raster}')\n",
    "        ax[num_col * idx + 1].set_title(f'Ground Truth Label')\n",
    "\n",
    "        # plot scenario 1\n",
    "        ax[num_col * idx + 2].imshow(lbl_overlap[region]['scenario_1_hand_lbl'][raster, :, :], cmap=ovrlp_cmap)\n",
    "        ax[num_col * idx + 2].set_title('Scenario 1 Label Overlap')\n",
    "\n",
    "        # plot scenario 2\n",
    "        ax[num_col * idx + 3].imshow(lbl_overlap[region]['scenario_2_hand_lbl'][raster, :, :], cmap=ovrlp_cmap)\n",
    "        ax[num_col * idx + 3].set_title('Scenario 2 Label Overlap')\n",
    "\n",
    "        # plot scenario 3\n",
    "        ax[num_col * idx + 4].imshow(lbl_overlap[region]['scenario_3_hand_lbl'][raster, :, :], cmap=ovrlp_cmap)\n",
    "        ax[num_col * idx + 4].set_title('Scenario 3 Label Overlap')\n",
    "\n",
    "        for axis in ax[: num_col * num_plot]:\n",
    "            scalebar = AnchoredSizeBar(\n",
    "                axis.transData,\n",
    "                100,\n",
    "                '100m',\n",
    "                'lower left',\n",
    "                pad=0.1,\n",
    "                color='black',\n",
    "                frameon=False,\n",
    "                size_vertical=1,\n",
    "                fontproperties=fontprops)\n",
    "\n",
    "            axis.add_artist(scalebar)\n",
    "            axis.set_yticks([])\n",
    "            axis.set_xticks([]);\n",
    "\n",
    "    # Create legend\n",
    "    checkerboard = np.zeros((512, 512))\n",
    "    checkerboard[0:256, 0:256] = 1\n",
    "    checkerboard[256:, 0:256] = 2\n",
    "    checkerboard[0:256, 256:] = 3\n",
    "    checkerboard[256:, 256:] = 4\n",
    "\n",
    "\n",
    "    ax[num_col * idx + 4 + 3].imshow(checkerboard, cmap=ovrlp_cmap)\n",
    "    ax[num_col * idx + 4 + 3].text(50, 128, \"True Positives\", fontsize=8.);\n",
    "    ax[num_col * idx + 4 + 3].text(50, 384, \"True Negatives\", fontsize=8.);\n",
    "    ax[num_col * idx + 4 + 3].text(290, 128, \"False Positives\", fontsize=8.);\n",
    "    ax[num_col * idx + 4 + 3].text(290, 384, \"False Negatives\", fontsize=8.);\n",
    "    ax[num_col * idx + 4 + 3].set_yticks([])\n",
    "    ax[num_col * idx + 4 + 3].set_xticks([]);\n",
    "\n",
    "    ind_to_del = [1, 2, 4, 5]\n",
    "    for ind in ind_to_del:\n",
    "        fig.delaxes(ax[num_col * idx + 4 + ind])\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Overlap for Region: USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ovrlp_lbl_pth = \"ovrlp_lbl_pth\"\n",
    "region = \"USA\"\n",
    "\n",
    "idx_USA = [1, 3, 5, 8, 22]\n",
    "fig_USA = display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                              lbl_ovrlap_by_region,\n",
    "                              X_test_hand_lbl_by_region,\n",
    "                              num_plot=len(idx_USA),\n",
    "                              region='USA',\n",
    "                              indices=idx_USA)\n",
    "\n",
    "# save\n",
    "#fig_USA.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Label Overlap for Region: Mekong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = \"Mekong\"\n",
    "idx_Mekong = [1, 2, 5, 7, 8]\n",
    "fig_Mekong = display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                                 lbl_ovrlap_by_region,\n",
    "                                 X_test_hand_lbl_by_region,\n",
    "                                 num_plot=len(idx_Mekong),\n",
    "                                 region=region,\n",
    "                                 indices=idx_Mekong)\n",
    "\n",
    "#fname = f\"{ovrlp_lbl_pth}\\\\{region}_lbl_ovrlp.pdf\"\n",
    "#fig_Mekong.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Label Overlap for Region: Bolivia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "idx_Bolivia = [1, 2, 3, 4, 5]\n",
    "region = \"Bolivia\"\n",
    "fig_Bol = display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                              lbl_ovrlap_by_region,\n",
    "                              X_test_hand_lbl_by_region,\n",
    "                              num_plot=len(idx_Bolivia),\n",
    "                              region=region,\n",
    "                              indices=idx_Bolivia)\n",
    "\n",
    "#fname = f\"{ovrlp_lbl_pth}\\\\{region}_lbl_ovrlp.pdf\"\n",
    "#fig_Bol.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Label Overlap for Region: Paraguay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = 'Paraguay'\n",
    "idx_Paraguay = [0, 1, 2, 6, 7]\n",
    "fig_Par = display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                              lbl_ovrlap_by_region,\n",
    "                              X_test_hand_lbl_by_region,\n",
    "                              num_plot=len(idx_Paraguay),\n",
    "                              region=region,\n",
    "                              indices=idx_Paraguay)\n",
    "\n",
    "#fname = f\"{ovrlp_lbl_pth}\\\\{region}_lbl_ovrlp.pdf\"\n",
    "#fig_Par.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Label Overlap for Region: India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = \"India\"\n",
    "idx_India = [0, 2, 4, 6, 23]\n",
    "fig_Ind =display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                             lbl_ovrlap_by_region,\n",
    "                             X_test_hand_lbl_by_region,\n",
    "                             num_plot=len(idx_India),\n",
    "                             region=region,\n",
    "                             indices=idx_India)\n",
    "\n",
    "#fname = f\"{ovrlp_lbl_pth}\\\\{region}_lbl_ovrlp.pdf\"\n",
    "#fig_Ind.savefig(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Label Overlap for Region: Sri-Lanka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region = \"Sri-Lanka\"\n",
    "idx_Sri_Lanka = [8, 9, 11, 16, 21]\n",
    "fig_Sri = display_lbl_overlap(Y_true_hand_lbl_by_region,\n",
    "                              lbl_ovrlap_by_region,\n",
    "                              X_test_hand_lbl_by_region,\n",
    "                              num_plot=len(idx_Sri_Lanka),\n",
    "                              region=region,\n",
    "                              indices=idx_Sri_Lanka)\n",
    "\n",
    "#fname = f\"{ovrlp_lbl_pth}\\\\{region}_lbl_ovrlp.pdf\"\n",
    "#fig_Sri.savefig(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
